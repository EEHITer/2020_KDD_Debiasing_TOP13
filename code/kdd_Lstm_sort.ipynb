{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import gc\n",
    "from collections import defaultdict  \n",
    "import math  \n",
    "import Standard_itemcf\n",
    "import Yu_itemcf_TimeSeries\n",
    "import Bipartite_network_CF\n",
    "import userCF\n",
    "import MyEvaluation\n",
    "import random\n",
    "from random import sample\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.sparse as sp\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation,Input,Convolution1D,Conv1D,GlobalMaxPooling1D,MaxPooling1D,Flatten,concatenate,Embedding,GRU,Lambda, LSTM, TimeDistributed\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model,Model\n",
    "from keras.layers import Dense, Embedding, LSTM,Flatten,BatchNormalization\n",
    "from gensim.models import word2vec \n",
    "import keras\n",
    "import time\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "pd.set_option('display.max_rows',200)\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#is_offline = True\n",
    "now_phase = 9\n",
    "train_path = '../input/underexpose_train'  \n",
    "test_path = '../input/underexpose_test' \n",
    "output_path='../output/'\n",
    "#eval = MyEvaluation.MyEvaluation(now_phase=now_phase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the items click sequence to pretrain the i2v embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = []\n",
    "tmp_all = pd.DataFrame()\n",
    "for c in range(6 + 1):  \n",
    "    click_train = pd.read_csv(train_path + '/underexpose_train_click-{}.csv'.format(c), header=None,  names=['user_id', 'item_id', 'time'])  \n",
    "    click_test = pd.read_csv(test_path + '/underexpose_test_click-{}.csv'.format(c), header=None,  names=['user_id', 'item_id', 'time'])  \n",
    "    tmp = click_train.append(click_test,ignore_index=True).sort_values('time')\n",
    "    tmp_all = tmp_all.append(tmp,ignore_index=True)\n",
    "    # 提取用户点击序列，并构成文本\n",
    "tmp_all.drop_duplicates(inplace=True)\n",
    "tmp_doc = tmp_all.groupby(['user_id'])['item_id'].agg({list}).reset_index()['list'].values.tolist()\n",
    "if len(doc)==0 : \n",
    "    doc =tmp_doc\n",
    "else:\n",
    "    doc.extend(tmp_doc)\n",
    "    \n",
    "\n",
    "# 导入 Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 转为字符串型才能进行训练\n",
    "for i in range(len(doc)):\n",
    "    doc[i] = [str(x) for x in doc[i]]\n",
    "embed_size=128\n",
    "model = Word2Vec(doc, size=embed_size, window=50, min_count=3, sg=1, hs=0,negative = 15,iter=200, seed=2020)\n",
    "\n",
    "# 训练结果提取\n",
    "values = set(tmp_all['item_id'].values)\n",
    "w2v=[]\n",
    "\n",
    "for v in values:\n",
    "    try:\n",
    "        a = [int(v)]\n",
    "        a.extend(model[str(v)])\n",
    "        w2v.append(a)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "out_df = pd.DataFrame(w2v)\n",
    "out_df.columns = ['item_id'] + ['item_vec'+str(i) for i in range(embed_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the positive samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_click = pd.DataFrame()  \n",
    "for c in range(7,now_phase + 1):  \n",
    "    click_train = pd.read_csv(train_path + '/underexpose_train_click-{}.csv'.format(c), header=None,  names=['user_id', 'item_id', 'time'])  \n",
    "    click_test = pd.read_csv(test_path + '/underexpose_test_click-{}.csv'.format(c), header=None,  names=['user_id', 'item_id', 'time'])  \n",
    "    all_click = click_train.append(click_test)  \n",
    "    whole_click = whole_click.append(all_click)  \n",
    "\n",
    "whole_click = whole_click.drop_duplicates(subset=['user_id','item_id','time'])\n",
    "whole_click = whole_click.sort_values('time',ascending=True)   \n",
    "whole_click = whole_click.reset_index(drop=True)\n",
    "for col in ['item_id','time']:#,'time_interval_cumsum']:\n",
    "    whole_click[col]=whole_click[col].astype(str)\n",
    "\n",
    "whole_item_df = whole_click.groupby('user_id').agg(\n",
    "                                item_list = pd.NamedAgg(column = 'item_id',aggfunc=(lambda x : list(x))),\n",
    "                                time_list = pd.NamedAgg(column = 'time',aggfunc=(lambda x : list(x))),\n",
    "                                ).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "positive_df = []\n",
    "last_n = 5\n",
    "for i in tqdm(whole_item_df.index):\n",
    "    user_id = whole_item_df.loc[i,'user_id']\n",
    "    item_list = whole_item_df.loc[i,'item_list']\n",
    "    time_list = whole_item_df.loc[i,'time_list']\n",
    "    if len(item_list)<=1: \n",
    "        continue\n",
    "    if len(item_list)<last_n: \n",
    "        for j in range(3,len(item_list)+1):\n",
    "            positive_df.append([user_id,' '.join(item_list[:j]),time_list[j-1]])\n",
    "    else:\n",
    "        for j in range(3,last_n):\n",
    "            positive_df.append([user_id,' '.join(item_list[:j]),time_list[j-1]])\n",
    "        for j in range(last_n,len(item_list)+1,1):\n",
    "            recent_item_list = item_list[j-last_n:j]\n",
    "            positive_df.append([user_id,' '.join(recent_item_list),time_list[j-1]])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_df_p = pd.DataFrame(positive_df,columns=['user_id','recent_items','time'])\n",
    "train_set_df_p['label'] = 1\n",
    "train_set_df_p['target_item'] = train_set_df_p['recent_items'].apply(lambda x : x.split(' ')[-1])\n",
    "train_set_df_p['recent_items'] = train_set_df_p['recent_items'].apply(lambda x : ' '.join(x.split(' ')[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_set_df_p = train_set_df_p.sample(frac=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the negative samples that 100 times more than positive samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "negative_ratio = 100\n",
    "negative_list = whole_click['item_id'].sample(len(train_set_df_p)*negative_ratio,replace=True).values.tolist()\n",
    "train_set_df = train_set_df_p[['user_id','time','target_item','label']].copy()\n",
    "for i in tqdm(range(negative_ratio)):\n",
    "    tmp_negative_list = negative_list[i*len(train_set_df_p):(i+1)*len(train_set_df_p)]\n",
    "    tmp_df = train_set_df_p[['user_id','time','target_item','label']].copy()\n",
    "    tmp_df['target_item']=tmp_negative_list\n",
    "    tmp_df['label'] = 0\n",
    "    train_set_df=train_set_df.append(tmp_df,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the user's feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = pd.read_csv('../input/underexpose_train/underexpose_user_feat.csv',names=['user_id', 'user_age_level', 'user_gender', 'user_city_level'])\n",
    "user_data = pd.concat([user_data,pd.get_dummies(user_data['user_gender'])],axis=1)\n",
    "user_data = user_data.drop_duplicates(subset=['user_id'])\n",
    "user_data.drop(columns=['user_gender'],inplace=True)\n",
    "\n",
    "train_set_df_p = train_set_df_p.merge(user_data,on='user_id',how='left')\n",
    "\n",
    "scaler_user = sklearn.preprocessing.StandardScaler()\n",
    "scaler_user.fit(train_set_df_p[['user_age_level', 'user_city_level','F', 'M']])\n",
    "train_set_df_p[['user_age_level', 'user_city_level','F', 'M']] = scaler_user.transform(train_set_df_p[['user_age_level', 'user_city_level','F', 'M']])\n",
    "\n",
    "train_set_df_p['user_age_level'].fillna(0,inplace=True)\n",
    "train_set_df_p['user_city_level'].fillna(0,inplace=True)\n",
    "train_set_df_p['F'].fillna(0,inplace=True)\n",
    "train_set_df_p['M'].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the last click in different phases. At last we will use them as reference to validade the model's effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = pd.DataFrame()\n",
    "for c in range(now_phase + 1):  \n",
    "    tmp_query = pd.read_csv(test_path + '/underexpose_test_qtime-{}.csv'.format(c), header=None,  names=['user_id', 'query_time'])  \n",
    "    test_query = test_query.append(tmp_query,ignore_index=True)\n",
    "    \n",
    "whole_click=whole_click.sort_values(by =['user_id','time'],ascending=False).reset_index(drop=True)    \n",
    "whole_click['time']=whole_click['time'].astype('float64')\n",
    "recent_items_df = pd.DataFrame()\n",
    "for i in tqdm(test_query.index):\n",
    "    user = test_query.loc[i,'user_id']\n",
    "    time = test_query.loc[i,'query_time']\n",
    "    time_left = str(whole_click.loc[(whole_click['user_id']==user)&(whole_click['time']<time)]['time'].max())\n",
    "    time_right =  str(whole_click.loc[(whole_click['user_id']==user)&(whole_click['time']>time)]['time'].min())\n",
    "    recent_items_df =recent_items_df.append([{'user_id':user,'time':time_left,'isval':1},{'user_id':user,'time':time_right,'isval':2}],ignore_index=True)\n",
    "recent_items_df = recent_items_df[recent_items_df['time']!='nan']\n",
    "train_set_df = train_set_df.merge(recent_items_df,on=['user_id','time'],how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data set into train set and valid set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_df = train_set_df[train_set_df['isval']==1].reset_index(drop=True)\n",
    "train_set_df = train_set_df[train_set_df['isval'].isnull()].reset_index(drop=True)\n",
    "\n",
    "val_data_ratio = 20\n",
    "random.seed(2020)\n",
    "sample_ind = sample(range(train_set_df.label.sum()),train_set_df.label.sum()//val_data_ratio)\n",
    "train_sample_ind = list(set(list(range(train_set_df.label.sum()))) - set(sample_ind))\n",
    "val_sample_ind = sample_ind[:len(sample_ind)]\n",
    "\n",
    "val_sample_ind_all = []\n",
    "for i in range(negative_ratio+1):\n",
    "    tmp_ind = np.add(val_sample_ind,train_set_df.label.sum()*i)\n",
    "    val_sample_ind_all.extend(tmp_ind)\n",
    "\n",
    "train_sample_ind_all = []\n",
    "for i in range(negative_ratio+1):\n",
    "    tmp_ind = np.add(train_sample_ind,train_set_df.label.sum()*i)\n",
    "    train_sample_ind_all.extend(tmp_ind)\n",
    "    \n",
    "\n",
    "test_len = test_set_df.shape[0]\n",
    "train_set_df = train_set_df.append(test_set_df,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_item_df['item_list'] = whole_item_df['item_list'].apply(lambda x : ' '.join([str(i) for i in x]))\n",
    "max_word_len = whole_click.item_id.nunique()\n",
    "tokenizer = Tokenizer(num_words=max_word_len, split=' ')\n",
    "tokenizer.fit_on_texts(whole_item_df['item_list'])\n",
    "recent_items_list = pad_sequences(tokenizer.texts_to_sequences(train_set_df_p['recent_items']))\n",
    "train_set_df_p['recent_items5'] = recent_items_list.tolist()\n",
    "train_set_df['target_item'] = pad_sequences(tokenizer.texts_to_sequences(train_set_df['target_item'])).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = out_df.set_index('item_id')\n",
    "max_feaures = max_word_len\n",
    "embedding_matrix = np.zeros((max_feaures+1, embed_size))\n",
    "word_index_list = list(tokenizer.word_index.keys())\n",
    "for ind in range(len(word_index_list)):\n",
    "    word_ = word_index_list[ind]\n",
    "    word = int(word_)\n",
    "    if word not in out_df.index:\n",
    "        continue\n",
    "    embedding_matrix[tokenizer.word_index[word_]] = out_df.loc[word].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_set_df = train_set_df[-test_len:]\n",
    "val_set_df = train_set_df.loc[val_sample_ind_all]\n",
    "train_set_df = train_set_df.loc[train_sample_ind_all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the duplicate items in different set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_df['target']=train_set_df['target_item'].apply(lambda x :x[0])\n",
    "train_set_df.drop_duplicates(subset=['user_id','time','target'],keep='first',inplace=True)\n",
    "train_set_df.drop(columns=['target','isval'],inplace = True)\n",
    "\n",
    "val_set_df['target']=val_set_df['target_item'].apply(lambda x :x[0])\n",
    "val_set_df.drop_duplicates(subset=['user_id','time','target'],keep='first',inplace=True)\n",
    "val_set_df.drop(columns=['target','isval'],inplace = True)\n",
    "\n",
    "test_set_df['target']=test_set_df['target_item'].apply(lambda x :x[0])\n",
    "test_set_df.drop_duplicates(subset=['user_id','time','target'],keep='first',inplace=True)\n",
    "test_set_df.drop(columns=['target','isval'],inplace = True)\n",
    "\n",
    "train_set_df = train_set_df.reset_index(drop=True)\n",
    "val_set_df = val_set_df.reset_index(drop=True)\n",
    "test_set_df = test_set_df.reset_index(drop=True)\n",
    "\n",
    "train_set_df_p.drop(columns=['target_item','label'],inplace=True,errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator:\n",
    "    def __init__(self, data, batch_size=128):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = len(self.data) // self.batch_size\n",
    "        if len(self.data) % self.batch_size != 0:\n",
    "            self.steps += 1\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        global train_set_df_p\n",
    "        while True:\n",
    "            train_set = self.data\n",
    "            idxs = list(range(len(self.data)))\n",
    "            np.random.shuffle(idxs)\n",
    "            index_list = []\n",
    "            X1,  X3 ,Y = [], [], []\n",
    "            for c, i in enumerate(idxs):\n",
    "                index_list.append(i)\n",
    "                if len(index_list) == self.batch_size or i == idxs[-1]:\n",
    "                    tmp_df = train_set.loc[index_list]\n",
    "                    tmp_df = tmp_df.merge(train_set_df_p,on=['user_id','time'])\n",
    "                    tmp_df['recent_items5'] = tmp_df['recent_items5']+tmp_df['target_item']\n",
    "                    X1 = np.array(tmp_df['recent_items5'].values.tolist())\n",
    "                    X3 = np.array(tmp_df[['user_age_level', 'user_city_level','F', 'M']].values.tolist())\n",
    "                    Y = np.array(tmp_df['label'].values.tolist())\n",
    "                    yield [X1,X3], Y\n",
    "                    tmp_df = pd.DataFrame()\n",
    "                    X1, X3 ,Y = [],  [] ,[]\n",
    "                    index_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the LSTM sort model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "seq = Input(shape=[last_n])\n",
    "emb = Embedding(\n",
    "           len(embedding_matrix),       #表示文本数据中词汇的取值可能数,从语料库之中保留多少个单词。 因为Keras需要预留一个全零层， 所以+1\n",
    "           embed_size,              # 嵌入单词的向量空间的大小。它为每个单词定义了这个层的输出向量的大小\n",
    "           weights=[embedding_matrix],   #构建一个[num_words, EMBEDDING_DIM]的矩阵,然后遍历word_index，将word在W2V模型之中对应vector复制                                        过来。换个方式说：embedding_matrix 是原始W2V的子集，排列顺序按照Tokenizer在fit之后的词顺序。作为权                                      重喂给Embedding Layer\n",
    "           input_length=last_n,        # 输入序列的长度，也就是一次输入带有的词汇个数\n",
    "           trainable=False            # 我们设置 trainable = False，代表词向量不作为参数进行更新\n",
    "           )(seq)\n",
    "lstm_1 = LSTM(embed_size, input_shape=(last_n, embed_size),return_sequences=True)(emb)\n",
    "lstm_2 = LSTM(embed_size)(lstm_1)\n",
    "input_2 = Input(shape=[4,])\n",
    "merge = concatenate([lstm_2,input_2])\n",
    "mlp = Dense(units=100,activation='relu')(merge)\n",
    "mlp = Dropout(0.3)(mlp)\n",
    "mlp=  BatchNormalization()(mlp)\n",
    "output = Dense(units=1,activation='sigmoid')(mlp)\n",
    "\n",
    "model = Model([seq,input_2],output)\n",
    "\n",
    "model.compile( optimizer=keras.optimizers.RMSprop(1e-3),loss=keras.losses.BinaryCrossentropy(),metrics=[keras.metrics.AUC()])\n",
    "\n",
    "filepath = \"sort_lstm_.h5\" \n",
    "monitor_name = 'val_auc_1'\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath, monitor=monitor_name, verbose=1, save_best_only=True, mode='max')\n",
    "reduce_lr2 = ReduceLROnPlateau(\n",
    "    monitor=monitor_name, factor=0.8, patience=1, min_lr=0.0001, verbose=1)\n",
    "earlystopping2 = EarlyStopping(\n",
    "    monitor=monitor_name, min_delta=0.0001, patience=2, verbose=1, mode='max')\n",
    "callbacks2 = [checkpoint, earlystopping2 ,reduce_lr2]\n",
    "train_D = data_generator(train_set_df,batch_size=8192)\n",
    "valid_D = data_generator(val_set_df,batch_size=8192)\n",
    "model.fit_generator(train_D.__iter__(),validation_data=valid_D.__iter__(),validation_steps=len(valid_D),steps_per_epoch=len(train_D),epochs=50, shuffle=True,callbacks=callbacks2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_D = data_generator(test_set_df.reset_index(drop=True),batch_size=8192)\n",
    "model.evaluate_generator(test_D.__iter__(),steps=len(test_D))\n",
    "model = load_model('./sort_lstm_.h5',compile=False)\n",
    "model.compile( optimizer=keras.optimizers.RMSprop(1e-3),loss=keras.losses.BinaryCrossentropy(),metrics=[keras.metrics.AUC()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the recall items "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(train_set_df , query_df,whole_click,isval=False):\n",
    "    recent_items_df = pd.DataFrame()\n",
    "    last_n=5\n",
    "    for i in tqdm(query_df.index):\n",
    "        user = query_df.loc[i,'user_id']\n",
    "        time = query_df.loc[i,'query_time']\n",
    "        phase = query_df.loc[i,'phase']\n",
    "        recent_items = ' '.join([str(i) for i in  whole_click.loc[(whole_click['user_id']==user)&(whole_click['time']<time)&(whole_click['phase']==phase)]['item_id'][:last_n-1].values.tolist()[::-1]])\n",
    "        recent_items_df = recent_items_df.append({'user_id':user,'query_time':time,'recent_items_5':recent_items,'phase':phase},ignore_index=True)\n",
    "    train_set_df = train_set_df.merge(recent_items_df,on=['user_id','query_time','phase'])\n",
    "    train_set_df['recent_items_5'] = train_set_df['recent_items_5']+' '+ train_set_df['item_id'].astype(str)\n",
    "#     tokenizer = joblib.load('./tokenizer.m')\n",
    "#     scaler_user = joblib.load('./user_info_std_scalar.m')\n",
    "    user_data = pd.read_csv('../input/underexpose_train/underexpose_user_feat.csv',names=['user_id', 'user_age_level', 'user_gender', 'user_city_level'])\n",
    "    user_data = pd.concat([user_data,pd.get_dummies(user_data['user_gender'])],axis=1)\n",
    "    user_data = user_data.drop_duplicates(subset=['user_id'])\n",
    "    user_data.drop(columns=['user_gender'],inplace=True)\n",
    "    train_set_df = train_set_df.merge(user_data,on='user_id',how='left')\n",
    "    train_set_df[['user_age_level', 'user_city_level','F', 'M']] = scaler_user.transform(train_set_df[['user_age_level', 'user_city_level','F', 'M']])\n",
    "    train_set_df['user_age_level'].fillna(0,inplace=True)\n",
    "    train_set_df['user_city_level'].fillna(0,inplace=True)\n",
    "    train_set_df['F'].fillna(0,inplace=True)\n",
    "    train_set_df['M'].fillna(0,inplace=True)\n",
    "    X = tokenizer.texts_to_sequences(train_set_df['recent_items_5'])\n",
    "    X = pad_sequences(X)\n",
    "    if isval:\n",
    "        return X,train_set_df[['user_age_level', 'user_city_level','F', 'M']],train_set_df['label']\n",
    "        #return train_set_df\n",
    "    else:\n",
    "        return X,train_set_df[['user_age_level', 'user_city_level','F', 'M']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_recall = pd.read_pickle('../user_data/data/recall.pkl')\n",
    "final_recall = final_recall[['user_id', 'phase', 'query_time', 'item_id', 'label']]\n",
    "train_set_df = final_recall[~final_recall['label'].isnull()].reset_index(drop=True)\n",
    "test_set_df = final_recall[final_recall['label'].isnull()].reset_index(drop=True)\n",
    "query_df = train_set_df[train_set_df['label']==1]\n",
    "\n",
    "now_phase = 9\n",
    "whole_click = pd.DataFrame()  \n",
    "for c in range(7,now_phase + 1):  \n",
    "    click_train = pd.read_csv(train_path + '/underexpose_train_click-{}.csv'.format(c), header=None,  names=['user_id', 'item_id', 'time'])  \n",
    "    click_test = pd.read_csv(test_path + '/underexpose_test_click-{}.csv'.format(c), header=None,  names=['user_id', 'item_id', 'time'])  \n",
    "    all_click = click_train.append(click_test)  \n",
    "    all_click['phase']=c\n",
    "    whole_click = whole_click.append(all_click)  \n",
    "whole_click = whole_click.drop_duplicates(subset=['user_id','item_id','time','phase'])\n",
    "whole_click = whole_click.sort_values(by =['user_id','time'],ascending=False).reset_index(drop=True)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1,X_2,y = get_feature(train_set_df,query_df,whole_click,isval=True)\n",
    "train_set_df['pred'] = lstm_model.predict([X_1,X_2],batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = test_set_df.drop_duplicates(['user_id','phase','query_time'])\n",
    "test_x_1,test_x_2 =get_feature(whole_click=whole_click,train_set_df=test_set_df,query_df=test_query,isval=False,)\n",
    "test_set_df['pred'] = lstm_model.predict([test_x_1,test_x_2 ],batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = train_set_df.append(test_set_df,ignore_index=True)\n",
    "data_all.to_pickle('../user_data/data/nn/nn2.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
